# üöÄ Prueba T√©cnica ‚Äì Operaciones 2025  

[![Tests](https://github.com/JuanC101195/ops_prueba2025/actions/workflows/tests.yml/badge.svg)](https://github.com/JuanC101195/ops_prueba2025/actions/workflows/tests.yml)  
![Version](https://img.shields.io/badge/version-v1.0.0-blue)  
![License](https://img.shields.io/badge/license-MIT-lightgrey)  

üìå Pipeline para procesamiento, normalizaci√≥n y geocodificaci√≥n de direcciones.  

---

## üìë Contenido
- Requisitos  
- Uso  
- Archivos generados  
- Validaci√≥n del filtro  
- Ejemplo de salida  
- Mapas  
- Tests  
- Uso con AWS S3  
- Autor  

---

## üì¶ Requisitos
- Python 3.10+  
- Cuenta de Google Cloud con **API Key de Geocoding**  
- (Opcional) AWS S3 configurado si se usa la subida de archivos  

Instalar dependencias:  

```bash
pip install -r requirements.txt
```

Configurar variables de entorno (`.env`):  

```bash
GOOGLE_API_KEY=your_api_key_here
S3_BUCKET=
S3_PREFIX=
```

**Importante:**  
- Las credenciales no se incluyen en este repo (por seguridad).  
- Si desea ejecutar con S3, debe configurar sus propias credenciales.  

Ejecutar el pipeline:  

```bash
python -m app.main run   --inputs ./sample_data/ejemplo.txt   --workdir ./workdir   --threshold 90
```

Par√°metros:  
- `--inputs`: archivo(s) de entrada (PDF o TXT)  
- `--workdir`: directorio de salida  
- `--threshold`: umbral de similitud (default 90)  
- `--s3`: opcional, subir a S3 antes de procesar  

---

## üìÇ Archivos generados
En el directorio `workdir/` encontrar√°s:  

- `homonimos.csv` / `.xlsx`: lista completa de hom√≥nimos  
- `homonimos_filtrados.csv` / `.xlsx`: hom√≥nimos con score ‚â•90  
- `resultados.csv` / `.xlsx`: direcciones geocodificadas  
- `resultados_unicos.csv` / `.xlsx`: resultados √∫nicos por direcci√≥n formateada  
- `resultados.db`: base de datos SQLite con 3 tablas (`homonimos_filtrados`, `geocoding`, `geocoding_unicos`)  
- `mapa.html` y `mapa_unicos.html`: mapas interactivos con pines  

---

## üß™ Validaci√≥n del filtro
Script incluido: `validate_filter.py`  

Ejecutar:  

```bash
python ./validate_filter.py
```

Resultado esperado:  
- Los 60 hom√≥nimos buenos pasan (score=100).  
- Casos artificiales ‚Äúmalos‚Äù no pasan (score <90).  

---

## üìä Ejemplo de salida
```text
Umbral = 90
PASARON: 60 de 65
NO PASARON: 5

--- Ejemplos que PASAN ---
[GOOD] Carrera 70 # 26A - 33  -> score=100.0
[GOOD] Carrera 70 No 26A 33   -> score=100.0

--- Ejemplos que NO PASAN ---
[BAD] Av 70 # 26A - 33, Bogota  -> score=83.3
```

---

## üó∫Ô∏è Mapas
Abrir los mapas generados en navegador:  

```bash
Start-Process ./workdir/mapa.html
Start-Process ./workdir/mapa_unicos.html
```

Cada pin muestra:  
- Direcci√≥n formateada  
- Coordenadas (lat, lng)  
- Score de similitud  

---

## üß™ Tests
Ejecutar pruebas unitarias:  

```bash
pytest -q
```

Incluye pruebas de similitud y validaci√≥n del filtro.  
Tests relacionados con S3 se saltan autom√°ticamente si no hay credenciales configuradas.  

---

## üìã Hom√≥nimos filtrados (primeras 10 filas de ejemplo)

| original              | homonimo              | score |
|-----------------------|-----------------------|-------|
| Carrera 70 # 26A - 33 | Carrera 70 # 26A - 33 | 100.0 |
| Carrera 70 # 26A - 33 | Carrera 70 # 26A 33   | 100.0 |
| Carrera 70 # 26A - 33 | Carrera 70 26A - 33   | 100.0 |
| Carrera 70 # 26A - 33 | Carrera 70 26A 33     | 100.0 |
| Carrera 70 # 26A - 33 | Carrera 70 No 26A - 33| 100.0 |
| Carrera 70 # 26A - 33 | Carrera 70 No 26A 33  | 100.0 |
| Carrera 70 # 26A - 33 | Carrera 70 Nro 26A - 33|100.0 |
| Carrera 70 # 26A - 33 | Carrera 70 Nro 26A 33 | 100.0 |
| Carrera 70 # 26A - 33 | Carrera 70 Num 26A - 33|100.0 |
| Carrera 70 # 26A - 33 | Carrera 70 Num 26A 33 | 100.0 |

> Nota: el archivo completo `homonimos_filtrados.csv` contiene todas las filas.  

---

## ‚òÅÔ∏è Uso con AWS S3
El pipeline tambi√©n soporta la carga de archivos desde AWS S3 antes de procesarlos.  

Configurar credenciales en PowerShell (solo usuarios finales):  

```powershell
setx AWS_ACCESS_KEY_ID "su_access_key"
setx AWS_SECRET_ACCESS_KEY "su_secret_key"
setx AWS_DEFAULT_REGION "us-east-2"
setx S3_BUCKET "nombre_de_su_bucket"
setx S3_PREFIX "uploads/"
```

Subir archivo de prueba a S3:  

```powershell
aws s3 cp ./sample_data/ejemplo.txt s3://nombre_de_su_bucket/uploads/ejemplo.txt
```

Ejecutar pipeline con S3:  

```bash
python -m app.main run   --inputs ejemplo.txt   --workdir ./workdir/from_s3   --threshold 90   --s3
```

El programa descargar√° autom√°ticamente el archivo desde S3, lo procesar√° y generar√°:  
- `homonimos.csv`, `resultados.csv`, `resultados_unicos.csv`  
- `mapa.html`, `mapa_unicos.html`  
- `resultados.db`  

---

## üë§ Autor
Proyecto desarrollado por [Juan Esteban Cardozo Rivera](https://github.com/JuanC101195)  
Prueba T√©cnica de Operaciones 2025.  

